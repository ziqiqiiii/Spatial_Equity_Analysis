{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6ca7509e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import osmnx as ox\n",
    "import pandas as pd\n",
    "import libpysal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15116d25",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "file_path = \"../data/data_cleaned/\"\n",
    "graph_file_path = f\"{file_path}singapore_walk_graph.graphml\"\n",
    "nodes_file_path  = f\"{file_path}singapore_walk_graph_nodes.geojson\"\n",
    "edges_file_path  = f\"{file_path}singapore_walk_graph_edges.geojson\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42316c2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Skipping field highway: unsupported OGR type: 5\n",
      "Skipping field lanes: unsupported OGR type: 5\n",
      "Skipping field maxspeed: unsupported OGR type: 5\n",
      "Skipping field name: unsupported OGR type: 5\n",
      "Skipping field access: unsupported OGR type: 5\n",
      "Skipping field service: unsupported OGR type: 5\n",
      "Skipping field tunnel: unsupported OGR type: 5\n",
      "Skipping field bridge: unsupported OGR type: 5\n",
      "Skipping field width: unsupported OGR type: 5\n",
      "Skipping field est_width: unsupported OGR type: 5\n"
     ]
    }
   ],
   "source": [
    "# CONSTANTS\n",
    "CRS_PROJ = \"EPSG:3414\"  # Singapore SVY21\n",
    "DISTANCE_THRESHOLD = 1600  # 1.5km max walking distance (~20 mins)\n",
    "\n",
    "# Load pre-saved projected graph and GeoJSONs\n",
    "G_proj = ox.load_graphml(graph_file_path)\n",
    "G_nodes_proj = gpd.read_file(nodes_file_path)\n",
    "G_edges_proj = gpd.read_file(edges_file_path)\n",
    "edges = G_edges_proj.union_all()\n",
    "\n",
    "# Project Graph to Meters only if needed\n",
    "graph_crs = G_proj.graph.get(\"crs\")\n",
    "if graph_crs is None or str(graph_crs) != CRS_PROJ:\n",
    "    G_proj = ox.project_graph(G_proj, to_crs=CRS_PROJ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ae13711",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"../data/data_cleaned/\"\n",
    "\n",
    "planning_area_file_path  = f\"{file_path}planning_area.geojson\"\n",
    "sport_facilities_file_path  = f\"{file_path}sport_facilities.geojson\"\n",
    "parks_file_path  = f\"{file_path}parks.geojson\"\n",
    "park_connector_file_path  = f\"{file_path}park_connector.geojson\"\n",
    "cycling_paths_file_path  = f\"{file_path}cycling_paths.geojson\"\n",
    "hdb_file_path  = f\"{file_path}hdb.geojson\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49b15d7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "planning_area = gpd.read_file(planning_area_file_path )\n",
    "sport_facilities  = gpd.read_file(sport_facilities_file_path)\n",
    "hdb = gpd.read_file(hdb_file_path)\n",
    "parks = gpd.read_file(parks_file_path)\n",
    "park_connector = gpd.read_file(park_connector_file_path)\n",
    "cycling_paths = gpd.read_file(cycling_paths_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2001e9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "planning_area = planning_area.to_crs(CRS_PROJ)\n",
    "subzone = subzone.to_crs(CRS_PROJ)\n",
    "sport_facilities = sport_facilities.to_crs(CRS_PROJ)\n",
    "park_connector = park_connector.to_crs(CRS_PROJ)\n",
    "parks = parks.to_crs(CRS_PROJ)\n",
    "cycling_paths = cycling_paths.to_crs(CRS_PROJ)\n",
    "hdb = hdb.to_crs(CRS_PROJ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5f6f524",
   "metadata": {},
   "outputs": [],
   "source": [
    "def facility_preprocessing(gdf_facilities):\n",
    "    \"\"\"\n",
    "    Prepares SportSG Facilities Data.\n",
    "    \"\"\"\n",
    "    # Ensure CRS\n",
    "    gdf_facilities = gdf_facilities.to_crs(CRS_PROJ)\n",
    "    \n",
    "    # Change the geometry from polygon Z to point (centroid )\n",
    "    gdf_facilities['geometry'] = gdf_facilities.geometry.centroid\n",
    "    \n",
    "    # Assign ID\n",
    "    gdf_facilities['facility_id'] = range(len(gdf_facilities))\n",
    "    \n",
    "    # Ensure we have a capacity column (Supply). \n",
    "    # Since floor area is not available, we'll assume 1\n",
    "    if 'capacity' not in gdf_facilities.columns:\n",
    "        gdf_facilities['capacity'] = 1\n",
    "        \n",
    "    return gdf_facilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d79bed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def population_preprocessing(gdf_pop):\n",
    "    \"\"\"\n",
    "    Prepares HDB Population Data.\n",
    "    \"\"\"\n",
    "    # Ensure CRS\n",
    "    gdf_pop = gdf_pop.to_crs(CRS_PROJ)\n",
    "    \n",
    "    # Ensure we have centroids\n",
    "    gdf_pop['centroid'] = gdf_pop.geometry.centroid\n",
    "    \n",
    "    # Assign ID\n",
    "    gdf_pop['pop_id'] = range(len(gdf_pop))\n",
    "    \n",
    "    # Create a buffer for the \"Euclidean Filter\" step later\n",
    "    gdf_pop['search_buffer'] = gdf_pop['centroid'].buffer(DISTANCE_THRESHOLD)\n",
    "    \n",
    "    return gdf_pop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b45e5203",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_nearest_nodes(gdf, G_proj, geometry, column_name = 'node_id'):\n",
    "    gdf = ox.nearest_nodes(G_proj, geometry.x, geometry.y)\n",
    "    gdf_pop[column_name] = pop_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8b9c4cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_index_within_distance(gdf_demand, gdf_supply):\n",
    "    \"\"\"\n",
    "    Only remain demand data that's within\n",
    "    supply's buffer zone\n",
    "    \"\"\"\n",
    "    tmp = gpd.sjoin(\n",
    "        gdf_supply, \n",
    "        gpd.GeoDataFrame(gdf_demand[['pop_id', 'search_buffer']], geometry='search_buffer'), \n",
    "        how='inner', \n",
    "        predicate='within'\n",
    "    )\n",
    "\n",
    "    return tmp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9f5a802",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_network_distances(gdf_pop, gdf_facilities, G_proj):\n",
    "    \"\"\"\n",
    "    Calculates network distance between Pop and Facilities\n",
    "    ONLY if they are within the Euclidean buffer (Optimization).\n",
    "    \"\"\"\n",
    "    \n",
    "    # 1. Pre-filter: Find pairs that are geographically close (Vectorized)\n",
    "    # This prevents calculating network paths for Punggol -> Jurong (waste of time)\n",
    "    joined = gpd.sjoin(\n",
    "        gdf_facilities, \n",
    "        gpd.GeoDataFrame(gdf_pop[['pop_id', 'search_buffer']], geometry='search_buffer'), \n",
    "        how='inner', \n",
    "        predicate='within'\n",
    "    )\n",
    "    # _get_index_within_distance(gdf_pop, gdf_facilities)\n",
    "    \n",
    "    print(\"join\")\n",
    "    display(joined.head())\n",
    "\n",
    "\n",
    "    # 2. Get Nearest Network Nodes\n",
    "    # To speed this up, calculating nodes for unique points only\n",
    "    \n",
    "    # For Population\n",
    "    pop_nodes = ox.nearest_nodes(G_proj, gdf_pop.centroid.x, gdf_pop.centroid.y)\n",
    "    gdf_pop['origin_node'] = pop_nodes\n",
    "    # _get_nearest_nodes(gdf_pop, G_proj, gdf_pop.centroid, column_name='origin_node')\n",
    "    \n",
    "    # For Facilities\n",
    "    fac_nodes = ox.nearest_nodes(G_proj, gdf_facilities.geometry.x, gdf_facilities.geometry.y)\n",
    "    gdf_facilities['dest_node'] = fac_nodes\n",
    "    # _get_nearest_nodes(gdf_facilities, G_proj, gdf_pop.geometry, column_name=''dest_node')\n",
    "\n",
    "    # Merge Node IDs into the pair list\n",
    "    pairs = joined.merge(gdf_pop[['pop_id', 'origin_node', 'population']], on='pop_id', how='left')\n",
    "    pairs = pairs.merge(\n",
    "        gdf_facilities[['facility_id', 'dest_node', 'capacity']],\n",
    "        on='facility_id',\n",
    "        how='left',\n",
    "        suffixes=(\"_x\", \"_y\")\n",
    "    )\n",
    "    \n",
    "    print(\"pairs\")\n",
    "    display(pairs.head())\n",
    "\n",
    "    # Normalize column names to avoid dest_node_x/dest_node_y conflicts\n",
    "    if 'dest_node' not in pairs.columns:\n",
    "        if 'dest_node_y' in pairs.columns:\n",
    "            pairs.rename(columns={'dest_node_y': 'dest_node'}, inplace=True)\n",
    "        elif 'dest_node_x' in pairs.columns:\n",
    "            pairs.rename(columns={'dest_node_x': 'dest_node'}, inplace=True)\n",
    "    if 'capacity' not in pairs.columns:\n",
    "        if 'capacity_y' in pairs.columns:\n",
    "            pairs.rename(columns={'capacity_y': 'capacity'}, inplace=True)\n",
    "        elif 'capacity_x' in pairs.columns:\n",
    "            pairs.rename(columns={'capacity_x': 'capacity'}, inplace=True)\n",
    "\n",
    "    # 3. Calculate Shortest Path (The Heavy Lifting)\n",
    "    # Using NetworkX bi-directional dijkstra (fastest for 1-to-1)\n",
    "    results = []\n",
    "    \n",
    "    for idx, row in pairs.iterrows():\n",
    "        try:\n",
    "            dist = nx.shortest_path_length(\n",
    "                G_proj, \n",
    "                source=row['origin_node'], \n",
    "                target=row['dest_node'], \n",
    "                weight='length'\n",
    "            )\n",
    "            \n",
    "            # Only keep if within threshold\n",
    "            if dist <= DISTANCE_THRESHOLD:\n",
    "                results.append({\n",
    "                    'pop_id': row['pop_id'],\n",
    "                    'facility_id': row['facility_id'],\n",
    "                    'distance': dist,\n",
    "                    'pop_demand': row['population'],\n",
    "                    'facility_supply': row['capacity']\n",
    "                })\n",
    "        except nx.NetworkXNoPath:\n",
    "            continue\n",
    "    result = pd.DataFrame(results)\n",
    "\n",
    "    print(\"result\")\n",
    "    display(result.head())\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f1226e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_m2sfca(df_od, distance_threshold):\n",
    "    \"\"\"\n",
    "    Calculate accessibility scores\n",
    "    \"\"\"\n",
    "    \n",
    "    # --- Gaussian Decay Function ---\n",
    "    # As distance increases, weight drops.\n",
    "    # At 0m, weight = 1. At Threshold, weight approaches 0.\n",
    "    v = -distance_threshold**2 / np.log(0.01) # 0.01 is the weight at the limit\n",
    "    df_od['weight'] = np.exp(-(df_od['distance'])**2 / v)\n",
    "    \n",
    "    # --- Step 1: Supply-to-Demand Ratio (Rj) ---\n",
    "    # How crowded is the facility?\n",
    "    \n",
    "    # Weighted Demand: Pop * DistanceWeight\n",
    "    df_od['weighted_demand'] = df_od['pop_demand'] * df_od['weight']\n",
    "    \n",
    "    # Sum of weighted demand per Facility\n",
    "    facility_demand_sum = df_od.groupby('facility_id')['weighted_demand'].sum().reset_index()\n",
    "    facility_demand_sum.rename(columns={'weighted_demand': 'total_demand_at_facility'}, inplace=True)\n",
    "    \n",
    "    # Merge back\n",
    "    df_od = df_od.merge(facility_demand_sum, on='facility_id')\n",
    "    \n",
    "    # Calculate Ratio Rj = Supply / Total Weighted Demand\n",
    "    df_od['Rj'] = df_od['facility_supply'] / df_od['total_demand_at_facility']\n",
    "    \n",
    "    # --- Step 2: Accessibility Score (Ai) ---\n",
    "    # Sum the ratios available to each resident\n",
    "    \n",
    "    # Weighted Ratio: Rj * DistanceWeight\n",
    "    df_od['weighted_Rj'] = df_od['Rj'] * df_od['weight']\n",
    "    \n",
    "    # Sum per Population Point\n",
    "    accessibility_scores = df_od.groupby('pop_id')['weighted_Rj'].sum().reset_index()\n",
    "    accessibility_scores.rename(columns={'weighted_Rj': '2sfca_score'}, inplace=True)\n",
    "    \n",
    "    return accessibility_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36ec6d07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Preprocess\n",
    "hdb = population_preprocessing(hdb)\n",
    "sport_facilities = facility_preprocessing(sport_facilities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9a8f80b",
   "metadata": {},
   "outputs": [],
   "source": [
    "hdb.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a48ba6ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "sport_facilities.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abd2a037",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Calculate Network Distances (The Slow Part)\n",
    "df_od_pairs = get_network_distances(hdb, sport_facilities, G_proj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c0c35cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Compute Score\n",
    "final_scores = calculate_m2sfca(df_od_pairs, DISTANCE_THRESHOLD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69b4c9a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Merge back to Map\n",
    "final_map = hdb.merge(final_scores, on='pop_id', how='left')\n",
    "\n",
    "# Fill NaN with 0 (areas with NO access)\n",
    "final_map['2sfca_score'] = final_map['2sfca_score'].fillna(0)\n",
    "\n",
    "# 6. Visualize\n",
    "final_map.plot(column='2sfca_score', legend=True, cmap='viridis', figsize=(10, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "654ba668",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 7. Export GeoDataFrame to GeoJSON (WGS84)\n",
    "# from pathlib import Path\n",
    "\n",
    "# # Prepare a copy with only one geometry column\n",
    "# final_map_export = final_map.copy()\n",
    "# active_geom_name = final_map_export.geometry.name\n",
    "# extra_geom_cols = [c for c, dt in final_map_export.dtypes.items() if dt == \"geometry\" and c != active_geom_name]\n",
    "# if extra_geom_cols:\n",
    "#     final_map_export = final_map_export.drop(columns=extra_geom_cols)\n",
    "\n",
    "# # Reproject to WGS84 for GeoJSON/web mapping\n",
    "# # GeoJSON (RFC 7946) requires WGS84 coordinates (EPSG:4326) in degrees; other CRSs are not allowed in the spec.\n",
    "# # Web compatibility: Web map libraries (Leaflet, Mapbox GL, deck.gl) expect lon/lat GeoJSON; EPSG:3414 will render in the wrong place or be rejected\n",
    "# final_map_wgs84 = final_map_export.to_crs(\"EPSG:4326\")\n",
    "\n",
    "# # Define output path\n",
    "# output_path = \"../data/data_cleaned/sportSG_2sfca.geojson\"\n",
    "# Path(Path(output_path).parent).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# # Save to GeoJSON\n",
    "# final_map_wgs84.to_file(output_path, driver=\"GeoJSON\")\n",
    "# print(f\"GeoJSON saved to: {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09e9dff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def parks_preprocessing(gdf_parks, destination='Entrance', min_separation=50, snap_buffer=20, dedup_buffer=25):\n",
    "#     \"\"\"\n",
    "#     Prepare Parks polygons for network-based accessibility:\n",
    "#     - Ensure projected CRS (meters)\n",
    "#     - Validate/fix geometries and keep valid areas\n",
    "#     - Compute area (sqm, ha) as a proxy for supply\n",
    "#     - Optionally convert to entrance points by intersecting park perimeters with road edges\n",
    "#     - Add identifiers and return a proper GeoDataFrame with active geometry\n",
    "\n",
    "#     Parameters\n",
    "#     ----------\n",
    "#     gdf_parks : GeoDataFrame (polygons)\n",
    "#     destination : {'Entrance', 'centroid'}\n",
    "#         - 'Entrance': derive access points along park boundaries where they touch network edges\n",
    "#         - 'centroid': use polygon centroids as access points\n",
    "#     min_separation : float (meters)\n",
    "#         Minimum distance for de-duplicating nearby entrances (used via component labeling)\n",
    "#     snap_buffer : float (meters)\n",
    "#         Buffer distance around park polygons to find nearby network edges for entrance creation\n",
    "#     dedup_buffer : float (meters)\n",
    "#         Buffer used to group nearby entrances for de-duplication\n",
    "#     \"\"\"\n",
    "#     # Ensure CRS\n",
    "#     gdf_parks = gdf_parks.to_crs(CRS_PROJ)\n",
    "\n",
    "#     # Drop empties and fix invalid geometries (buffer(0) trick)\n",
    "#     gdf_parks = gdf_parks[gdf_parks.geometry.notna() & ~gdf_parks.geometry.is_empty]\n",
    "#     gdf_parks[\"geometry\"] = gdf_parks.buffer(0)\n",
    "\n",
    "#     # Keep polygonal geometries only\n",
    "#     gdf_parks = gdf_parks[gdf_parks.geometry.geom_type.isin([\"Polygon\", \"MultiPolygon\"])]\n",
    "\n",
    "#     # Compute area (projected CRS ensures meters)\n",
    "#     gdf_parks[\"area\"] = gdf_parks.geometry.area\n",
    "#     gdf_parks = gdf_parks[gdf_parks[\"area\"] > 0]\n",
    "\n",
    "#     # Precompute centroids (useful for fallback or centroid option)\n",
    "#     gdf_parks[\"centroid\"] = gdf_parks.geometry.centroid\n",
    "\n",
    "#     # Assign a stable park_id\n",
    "#     gdf_parks = gdf_parks.reset_index(drop=True)\n",
    "#     gdf_parks[\"park_id\"] = np.arange(len(gdf_parks))\n",
    "\n",
    "#     green_access = None\n",
    "\n",
    "#     # Case 1: Use centroids directly\n",
    "#     if str(destination).lower() == 'centroid':\n",
    "#         ga = gdf_parks[[\"centroid\", \"area\", \"park_id\"]].rename(columns={\"centroid\": \"geometry\"}).copy()\n",
    "#         green_access = gpd.GeoDataFrame(ga, geometry=\"geometry\", crs=CRS_PROJ)\n",
    "#     # Case 2: Derive entrances from boundary-edge intersections\n",
    "#     else:\n",
    "#         entrance_points = []\n",
    "#         # Buffer the park polygons slightly to capture nearby network edges\n",
    "#         gdf_parks[\"_buffer\"] = gdf_parks.geometry.buffer(snap_buffer)\n",
    "\n",
    "#         for i, row in gdf_parks.iterrows():\n",
    "#             green_area = row[\"geometry\"].area\n",
    "#             inter = row[\"_buffer\"].boundary.intersection(edges)\n",
    "#             if inter.is_empty:\n",
    "#                 continue\n",
    "#             # Collect point geometries from intersection results\n",
    "#             if hasattr(inter, \"geoms\"):\n",
    "#                 pts = [g for g in inter.geoms if g.geom_type == \"Point\"]\n",
    "#             elif inter.geom_type == \"Point\":\n",
    "#                 pts = [inter]\n",
    "#             else:\n",
    "#                 pts = []\n",
    "#             for p in pts:\n",
    "#                 entrance_points.append({\"geometry\": p, \"area\": green_area, \"park_id\": row[\"park_id\"]})\n",
    "\n",
    "#         # Build GeoDataFrame from entrances or fallback to centroids if none found\n",
    "#         if len(entrance_points) == 0:\n",
    "#             # Fallback: use centroids to avoid empty result and downstream KeyError\n",
    "#             ga = gdf_parks[[\"centroid\", \"area\", \"park_id\"]].rename(columns={\"centroid\": \"geometry\"}).copy()\n",
    "#             green_access = gpd.GeoDataFrame(ga, geometry=\"geometry\", crs=CRS_PROJ)\n",
    "#         else:\n",
    "#             green_access = gpd.GeoDataFrame(entrance_points, geometry=\"geometry\", crs=CRS_PROJ)\n",
    "\n",
    "#         # De-duplicate entrances closer than dedup_buffer via fuzzy contiguity components\n",
    "#         if len(green_access) > 0:\n",
    "#             green_access[\"_dedup_buf\"] = green_access.geometry.buffer(dedup_buffer)\n",
    "#             w_e = libpysal.weights.fuzzy_contiguity(green_access[\"_dedup_buf\"]) if len(green_access) > 0 else None\n",
    "#             if w_e is not None:\n",
    "#                 green_access[\"entrance_components\"] = w_e.component_labels\n",
    "#                 green_access = green_access.drop_duplicates(subset=[\"entrance_components\"], keep='first').reset_index(drop=True)\n",
    "#             if \"_dedup_buf\" in green_access:\n",
    "#                 green_access = green_access.drop(columns=[\"_dedup_buf\"])  # cleanup helper\n",
    "#         if \"_buffer\" in gdf_parks:\n",
    "#             gdf_parks = gdf_parks.drop(columns=[\"_buffer\"])  # cleanup helper\n",
    "\n",
    "#     # If still empty (edge case), return an empty but well-formed GeoDataFrame\n",
    "#     if green_access is None or len(green_access) == 0:\n",
    "#         return gpd.GeoDataFrame(columns=[\"geometry\", \"area\", \"park_id\", \"facility_id\", \"capacity\"], geometry=\"geometry\", crs=CRS_PROJ)\n",
    "\n",
    "#     # Ensure required columns for downstream functions\n",
    "#     green_access = green_access.reset_index(drop=True)\n",
    "#     # Each access point acts as a facility\n",
    "#     green_access[\"facility_id\"] = np.arange(len(green_access))\n",
    "#     # Use park area as supply proxy; for multiple entrances per park, distribute area evenly\n",
    "#     if \"park_id\" in green_access.columns:\n",
    "#         counts = green_access.groupby(\"park_id\")[\"park_id\"].transform(\"count\")\n",
    "#         green_access[\"capacity\"] = green_access[\"area\"] / counts\n",
    "#     else:\n",
    "#         green_access[\"capacity\"] = green_access[\"area\"]\n",
    "\n",
    "#     return green_access"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa090128",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parks.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb4d85cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# green_access = parks_preprocessing(parks)\n",
    "# green_access.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bff9271",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_od_pairs = get_network_distances(hdb, green_access, G_proj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "153981bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# final_scores = calculate_m2sfca(df_od_pairs, DISTANCE_THRESHOLD)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "460a0df0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# final_map = hdb.merge(final_scores, on='pop_id', how='left')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd9db2b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# final_map['2sfca_score'] = final_map['2sfca_score'].fillna(0)\n",
    "# final_map.plot(column='2sfca_score', legend=True, cmap='viridis', figsize=(10, 10))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "geopandas",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
