{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f4ca75c2",
   "metadata": {},
   "source": [
    "# Facilities equity analysis (planning-area level)\n",
    "\n",
    "This notebook parses the SportSG GeoJSON, extracts facility counts from the HTML `Description` field, computes centroids, and (optionally) spatially joins to the MasterPlan planning-area polygons to produce per-planning-area supply metrics.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f6fce6b",
   "metadata": {},
   "source": [
    "## Setup: create and activate the conda environment (one-time)\n",
    "Run these commands in a terminal BEFORE executing heavy spatial cells: \n",
    "\n",
    "```bash\n",
    "conda env create -f environment.yml\n",
    "conda activate urban-transformer\n",
    "jupyter notebook\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ecc706e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "geopandas available: True\n",
      "beautifulsoup4 available: True\n",
      "DATA_DIR = /home/amber/Urban-Transformer/data\n",
      "OUT_DIR = /home/amber/Urban-Transformer/outputs\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: imports and environment checks\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import json\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "# geopandas/bs4 are optional: we will attempt to import and fall back gracefully\n",
    "try:\n",
    "    import geopandas as gpd\n",
    "    from shapely.geometry import shape\n",
    "    GEOPANDAS_OK = True\n",
    "except Exception:\n",
    "    GEOPANDAS_OK = False\n",
    "\n",
    "try:\n",
    "    from bs4 import BeautifulSoup\n",
    "    BS4_OK = True\n",
    "except Exception:\n",
    "    BS4_OK = False\n",
    "\n",
    "print('geopandas available:', GEOPANDAS_OK)\n",
    "print('beautifulsoup4 available:', BS4_OK)\n",
    "\n",
    "ROOT = Path('..').resolve() if Path('.').resolve().name == 'notebooks' else Path('.').resolve()\n",
    "DATA_DIR = ROOT / 'data'\n",
    "OUT_DIR = ROOT / 'outputs'\n",
    "OUT_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "print('DATA_DIR =', DATA_DIR)\n",
    "print('OUT_DIR =', OUT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fdd27803",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: helper to parse the HTML Description table (uses BeautifulSoup if available)\n",
    "def parse_description_table(html):\n",
    "    if not html:\n",
    "        return {}\n",
    "    if BS4_OK:\n",
    "        soup = BeautifulSoup(html, 'lxml')\n",
    "        table = soup.find('table')\n",
    "        if not table:\n",
    "            return {}\n",
    "        out = {}\n",
    "        for tr in table.find_all('tr'):\n",
    "            th = tr.find('th')\n",
    "            td = tr.find('td')\n",
    "            if th and td:\n",
    "                out[th.get_text(strip=True)] = td.get_text(strip=True)\n",
    "        return out\n",
    "    # fallback: simple regex extraction of <th>..</th><td>..</td> pairs\n",
    "    out = {}\n",
    "    pairs = re.findall(r'<th[^>]*>([^<]+)</th>*<td[^>]*>([^<]+)</td>', html, flags=re.I)\n",
    "    for k, v in pairs:\n",
    "        out[k.strip()] = v.strip()\n",
    "    return out\n",
    "\n",
    "def to_number(s):\n",
    "    if s is None:\n",
    "        return 0\n",
    "    s = str(s).strip()\n",
    "    if s == '':\n",
    "        return 0\n",
    "    s2 = re.sub(r'[^0-9.-]', '', s)\n",
    "    try:\n",
    "        if '.' in s2:\n",
    "            return float(s2)\n",
    "        return int(s2)\n",
    "    except Exception:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "68998a98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote parsed facility attributes to /home/amber/Urban-Transformer/outputs/parsed_sportsg_facilities.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>BADMINTON_</th>\n",
       "      <th>TABLE_TENN</th>\n",
       "      <th>TENNIS_COU</th>\n",
       "      <th>SWIMMING_C</th>\n",
       "      <th>WADING_POO</th>\n",
       "      <th>GYM</th>\n",
       "      <th>FOOTBALL_F</th>\n",
       "      <th>ATHLETICS_</th>\n",
       "      <th>centroid_lon</th>\n",
       "      <th>centroid_lat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Clementi Stadium</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Clementi Sports Centre</td>\n",
       "      <td>14</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Jurong West Sports Centre</td>\n",
       "      <td>11</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Kallang Basin Swimming Complex</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Kallang Sports Centre</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Katong Swimming Complex</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>AMK Swimming Complex</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Bishan Sports Centre</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Bukit Batok Swimming Complex</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Farrer Park Field and Tennis Centre</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  name  BADMINTON_  TABLE_TENN  TENNIS_COU  \\\n",
       "0                     Clementi Stadium           0           0           0   \n",
       "1               Clementi Sports Centre          14          10           0   \n",
       "2            Jurong West Sports Centre          11           3           2   \n",
       "3       Kallang Basin Swimming Complex           0           0           0   \n",
       "4                Kallang Sports Centre           0           0          14   \n",
       "5              Katong Swimming Complex           0           0           0   \n",
       "6                 AMK Swimming Complex           0           0           0   \n",
       "7                 Bishan Sports Centre           4           9           0   \n",
       "8         Bukit Batok Swimming Complex           0           0           0   \n",
       "9  Farrer Park Field and Tennis Centre           0           0           8   \n",
       "\n",
       "   SWIMMING_C  WADING_POO  GYM  FOOTBALL_F  ATHLETICS_ centroid_lon  \\\n",
       "0           0           0    0           1           1         None   \n",
       "1           0           1    1           0           0         None   \n",
       "2           0           1    1           1           1         None   \n",
       "3           0           1    0           0           0         None   \n",
       "4           0           0    0           1           1         None   \n",
       "5           0           1    0           0           0         None   \n",
       "6           0           1    0           0           0         None   \n",
       "7           0           1    1           1           1         None   \n",
       "8           0           1    0           0           0         None   \n",
       "9           0           0    0           1           0         None   \n",
       "\n",
       "  centroid_lat  \n",
       "0         None  \n",
       "1         None  \n",
       "2         None  \n",
       "3         None  \n",
       "4         None  \n",
       "5         None  \n",
       "6         None  \n",
       "7         None  \n",
       "8         None  \n",
       "9         None  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cell 3: parse facility GeoJSON into a DataFrame (non-spatial fallback works without geopandas)\n",
    "FAC_PATH = DATA_DIR / 'SportSGSportFacilitiesGEOJSON.geojson'\n",
    "assert FAC_PATH.exists(), f'Missing {FAC_PATH}'\n",
    "\n",
    "with open(FAC_PATH, 'r', encoding='utf8') as fh:\n",
    "    gj = json.load(fh)\n",
    "\n",
    "records = []\n",
    "for feat in gj.get('features', []):\n",
    "    props = feat.get('properties', {}) or {}\n",
    "    desc = props.get('Description') or props.get('description') or ''\n",
    "    attrs = parse_description_table(desc)\n",
    "    rec = {}\n",
    "    rec['name'] = attrs.get('SPORTS_CEN') or props.get('SPORTS_CEN') or props.get('Name') or ''\n",
    "    # extract some common numeric fields (if present)\n",
    "    for key in ['BADMINTON_', 'TABLE_TENN', 'TENNIS_COU', 'SWIMMING_C', 'WADING_POO', 'GYM', 'FOOTBALL_F', 'ATHLETICS_']:\n",
    "        if key in attrs:\n",
    "            rec[key] = to_number(attrs.get(key))\n",
    "        elif key in props:\n",
    "            rec[key] = to_number(props.get(key))\n",
    "        else:\n",
    "            rec[key] = 0\n",
    "    # geometry centroid (lon, lat) if geometry present\n",
    "    geom = feat.get('geometry')\n",
    "    if geom and geom.get('type') in ('Polygon', 'MultiPolygon'):\n",
    "        # compute centroid roughly by averaging coordinates (fast fallback)\n",
    "        coords = []\n",
    "        def collect_coords(g):\n",
    "            if isinstance(g, list):\n",
    "                for x in g:\n",
    "                    collect_coords(x)\n",
    "            elif isinstance(g, (int, float)):\n",
    "                pass\n",
    "        # safer approach: find the first list of coordinate tuples\n",
    "        try:\n",
    "            rings = geom.get('coordinates', [])\n",
    "            # dig until we find numeric pairs\n",
    "            def find_pairs(o):\n",
    "                if isinstance(o, list) and len(o) and isinstance(o[0], (list, tuple)):\n",
    "                    return o\n",
    "                if isinstance(o, list):\n",
    "                    for e in o:\n",
    "                        res = find_pairs(e)\n",
    "                        if res:\n",
    "                            return res\n",
    "                return None\n",
    "            pairs = find_pairs(rings) or []\n",
    "            xs = [p[0] for p in pairs if isinstance(p, (list, tuple)) and len(p)>=2]\n",
    "            ys = [p[1] for p in pairs if isinstance(p, (list, tuple)) and len(p)>=2]\n",
    "            if xs and ys:\n",
    "                rec['centroid_lon'] = sum(xs)/len(xs)\n",
    "                rec['centroid_lat'] = sum(ys)/len(ys)\n",
    "            else:\n",
    "                rec['centroid_lon'] = None\n",
    "                rec['centroid_lat'] = None\n",
    "        except Exception:\n",
    "            rec['centroid_lon'] = None\n",
    "            rec['centroid_lat'] = None\n",
    "    else:\n",
    "        rec['centroid_lon'] = None\n",
    "        rec['centroid_lat'] = None\n",
    "    records.append(rec)\n",
    "\n",
    "fac_df = pd.DataFrame(records)\n",
    "fac_out = OUT_DIR / 'parsed_sportsg_facilities.csv'\n",
    "fac_df.to_csv(fac_out, index=False)\n",
    "print('Wrote parsed facility attributes to', fac_out)\n",
    "fac_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c525ca03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading planning areas...\n",
      "Wrote spatial aggregation to /home/amber/Urban-Transformer/outputs/facilities_by_planning_area.geojson\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: optional spatial join (requires geopandas)\n",
    "if not GEOPANDAS_OK:\n",
    "    print('geopandas not available — skip spatial join. Create conda env and enable geopandas to run this cell.')\n",
    "else:\n",
    "    PLAN_PATH = DATA_DIR / 'MasterPlan2019PlanningAreaBoundaryNoSea.geojson'\n",
    "    assert PLAN_PATH.exists(), f'Missing {PLAN_PATH}'\n",
    "    print('Loading planning areas...')\n",
    "    plan_gdf = gpd.read_file(PLAN_PATH)\n",
    "    # try to extract a planning-area name from Description table where present\n",
    "    def get_plan_name(desc):\n",
    "        d = parse_description_table(desc)\n",
    "        for key in ('PLN_AREA_N', 'PLN_AREA_N'.upper(), 'PLN_AREA_N'.lower()):\n",
    "            if key in d:\n",
    "                return d[key]\n",
    "        return None\n",
    "    if 'planning_area' not in plan_gdf.columns:\n",
    "        # create a planning_area column heuristically\n",
    "        if 'Description' in plan_gdf.columns:\n",
    "            plan_gdf['planning_area'] = plan_gdf['Description'].apply(lambda x: get_plan_name(x) or '')\n",
    "        else:\n",
    "            plan_gdf['planning_area'] = plan_gdf.index.astype(str)\n",
    "    # load parsed facilities and convert to GeoDataFrame\n",
    "    pf = fac_df.copy()\n",
    "    pf = pf.dropna(subset=['centroid_lon', 'centroid_lat']).copy()\n",
    "    pf['geometry'] = gpd.points_from_xy(pf['centroid_lon'], pf['centroid_lat'])\n",
    "    pf_gdf = gpd.GeoDataFrame(pf, geometry='geometry', crs='EPSG:4326')\n",
    "    # ensure same CRS and spatial join\n",
    "    pf_gdf = pf_gdf.to_crs(plan_gdf.crs)\n",
    "    joined = gpd.sjoin(pf_gdf, plan_gdf[['planning_area', 'geometry']], how='left', predicate='within')\n",
    "    agg = joined.groupby('planning_area').agg(total_facilities=('name','count')).reset_index()\n",
    "    merged = plan_gdf.merge(agg, on='planning_area', how='left')\n",
    "    merged['total_facilities'] = merged['total_facilities'].fillna(0).astype(int)\n",
    "    merged_out = OUT_DIR / 'facilities_by_planning_area.geojson'\n",
    "    merged.to_file(merged_out, driver='GeoJSON')\n",
    "    print('Wrote spatial aggregation to', merged_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "982e30eb",
   "metadata": {},
   "source": [
    "## Alignment with Master Plan 2025\n",
    "\n",
    "This analysis supports Master Plan objectives such as promoting liveable and inclusive neighbourhoods, improving active mobility and sustainable transport, and ensuring equitable distribution of social infrastructure (sports facilities, cycling paths).\n",
    "\n",
    "The planning-area summaries below produce per-area facility counts and (where population data exists) simple supply rates. These can be used to identify gaps relative to Master Plan spatial strategies and to inform targeted facility or cycling-path investments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b4a9f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: quick alignment check with planning-area outputs\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "from pathlib import Path\n",
    "OUT_DIR = Path('..') / 'outputs' if Path('.').resolve().name == 'notebooks' else Path('outputs')\n",
    "pa_fp = OUT_DIR / 'facilities_by_planning_area.geojson'\n",
    "if not pa_fp.exists():\n",
    "    print('Missing', pa_fp, \"— run the spatial join cell (Cell 4) to create it.\")\n",
    "else:\n",
    "    pa = gpd.read_file(pa_fp)\n",
    "    # try find population column heuristically\n",
    "    pop_cols = [c for c in pa.columns if 'pop' in c.lower() or 'population' in c.lower()]\n",
    "    if pop_cols:\n",
    "        pop_col = pop_cols[0]\n",
    "        pa['fac_per_1000'] = pa['total_facilities'] / (pa[pop_col].replace(0, np.nan) / 1000)\n",
    "        print('Using population column:', pop_col)\n",
    "        display(pa[['planning_area','total_facilities', pop_col, 'fac_per_1000']].sort_values('fac_per_1000').head(10))\n",
    "    else:\n",
    "        print('No population column found in planning-area GeoJSON. Showing facility counts per planning area:')\n",
    "        display(pa[['planning_area','total_facilities']].sort_values('total_facilities').head(20))\n",
    "    print('\\nFacility count summary:')\n",
    "    display(pa['total_facilities'].describe())\n",
    "    # write a CSV summary for downstream use\n",
    "    out = OUT_DIR / 'facilities_by_pa_summary.csv'\n",
    "    pa[['planning_area','total_facilities']].to_csv(out, index=False)\n",
    "    print('Wrote summary to', out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a57ed43b",
   "metadata": {},
   "source": [
    "## Outcomes and deliverables (evidence of thorough understanding)\n",
    "\n",
    "Below is a comprehensive list of concrete outcomes this project will (or already does) produce, mapped to the datasets and analyses in this repository.\n",
    "\n",
    "### Data ingestion and cleaning\n",
    "- Parsed SportSG facilities with attributes extracted from HTML Description table → `outputs/parsed_sportsg_facilities.csv` (from Cell 3).\n",
    "- Valid centroids generated for facilities; handling of missing/invalid geometries documented.\n",
    "- Planning areas loaded from Master Plan 2019 boundaries; heuristic extraction of `planning_area` field from polygon metadata where needed (Cell 4).\n",
    "- CRS harmonisation to a projected CRS suitable for distance analysis (Singapore SVY21 EPSG:3414 recommended).\n",
    "- Cycling network KML inspected and prepared for use in accessibility metrics (conversion to GeoDataFrame and, where feasible, routable network graph).\n",
    "\n",
    "### Spatial aggregation and baseline supply\n",
    "- Spatial join of facilities to planning areas using point-in-polygon → `outputs/facilities_by_planning_area.geojson` (Cell 4).\n",
    "- Per planning area facility counts and summary statistics → `outputs/facilities_by_pa_summary.csv` (Cell 5).\n",
    "- Optional subzone-level aggregation (using `MasterPlan2019SubzoneBoundaryNoSeaGEOJSON.geojson`) to test Modifiable Areal Unit Problem (MAUP).\n",
    "\n",
    "### Accessibility metrics\n",
    "- Euclidean metrics per planning area:\n",
    "  - Nearest facility distance (meters) from area centroid.\n",
    "  - Counts of facilities within 400/800/1600 m buffers.\n",
    "- Network-aware metrics (if a routable cycling/street graph is available):\n",
    "  - Shortest-path distance along network to nearest facility.\n",
    "  - Cumulative opportunities (count of facilities within X minutes cycling).\n",
    "- Cycling infrastructure indicators: total cycling-path length within area, density per km².\n",
    "\n",
    "### Demographics and participation\n",
    "- Integration of population/household stats with the planning areas (e.g., age bands, income).\n",
    "- Participation metrics from \"Top Sports & Exercise – ...\" CSVs aligned to demographic strata (age/ethnicity/sex).\n",
    "- Derived rates: participants per group population; per-capita facility access by group (facilities per 1,000 group members).\n",
    "\n",
    "### Equity measurement and statistical testing\n",
    "- Inequality indices over access metrics (e.g., Gini, Concentration/Atkinson indices).\n",
    "- Hypothesis tests comparing access across demographic groups (ANOVA/Kruskal–Wallis, bootstrapped mean differences).\n",
    "- Identification and mapping of underserved areas (e.g., bottom 20% by access and/or participation).\n",
    "\n",
    "### Geospatial machine learning\n",
    "- Predictive models for participation rate or underserved classification using built-environment and accessibility features:\n",
    "  - Spatial regression (lag/error) via PySAL.\n",
    "  - Geographically Weighted Regression (GWR) via mgwr.\n",
    "  - Tree-based models (RandomForest/XGBoost) with spatial cross-validation.\n",
    "- Model interpretability: feature importances and SHAP value explanations; spatial mapping of local contributions.\n",
    "\n",
    "### Validation, sensitivity, and robustness\n",
    "- Spatial block cross-validation to reduce spatial leakage; report of spatial-CV vs random-CV performance.\n",
    "- Sensitivity to spatial unit (planning area vs subzone vs hex grid) and buffer thresholds (400/800/1600 m).\n",
    "- Robustness checks: outlier handling, imputation choices, and alternative access definitions (Euclidean vs network).\n",
    "\n",
    "### Visualizations and communication\n",
    "- Choropleths of facilities per 1,000 residents and nearest-distance surfaces by area.\n",
    "- Overlays of facilities and cycling paths for visual context.\n",
    "- Interactive maps (folium/kepler.gl) to explore access and equity by demographic group.\n",
    "- Summary dashboards/figures to support planning recommendations.\n",
    "\n",
    "### Policy alignment and impact\n",
    "- Direct alignment to Master Plan 2025 objectives: liveability, inclusivity, active mobility, and equitable social infrastructure distribution.\n",
    "- Actionable insights: list of candidate areas for new/expanded sports facilities or cycling-path improvements based on quantified gaps.\n",
    "\n",
    "### Reproducibility\n",
    "- Single-source notebook (`notebooks/facilities_equity_analysis.ipynb`) with documented steps and outputs saved under `outputs/`.\n",
    "- `environment.yml` updated with geospatial/ML dependencies for deterministic setup.\n",
    "- Intermediate artifacts stored as GeoJSON/CSV/Parquet to accelerate iteration and enable auditability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07b22d6d",
   "metadata": {},
   "source": [
    "## Datasets needed (core + optional)\n",
    "\n",
    "Below is a checklist of datasets required for the analysis, with purpose and the exact filenames found under `data/` in this repo.\n",
    "\n",
    "### Core geospatial datasets\n",
    "- Sport facilities (supply, geometry): `data/SportSGSportFacilitiesGEOJSON.geojson`\n",
    "- Planning area boundaries (analysis geography): `data/MasterPlan2019PlanningAreaBoundaryNoSea.geojson`\n",
    "- Subzone boundaries (alternative geography for MAUP checks): `data/MasterPlan2019SubzoneBoundaryNoSeaGEOJSON.geojson`\n",
    "- Cycling paths (active mobility network; for network distance/coverage): `data/CyclingPathNetworkKML.kml`\n",
    "\n",
    "### Demographics and participation (outcomes/explanatory)\n",
    "- Participation by age: \n",
    "  - `data/Top Sports & Exercise – 13 to 19 years old.csv`\n",
    "  - `data/Top Sports & Exercise – 20 to 39 years old.csv`\n",
    "  - `data/Top Sports & Exercise – 40 to 59 years old.csv`\n",
    "  - `data/Top Sports & Exercise – 60+ years old.csv`\n",
    "- Participation by ethnicity:\n",
    "  - `data/Top Sports & Exercise – Chinese.csv`\n",
    "  - `data/Top Sports & Exercise – Indians.csv`\n",
    "  - `data/Top Sports & Exercise – Malays.csv`\n",
    "- Participation by sex:\n",
    "  - `data/Top Sports & Exercise – Females.csv`\n",
    "- Socioeconomic / population denominators:\n",
    "  - Income (household): `data/ResidentHouseholdsbyMonthlyHouseholdIncomefromWork1andNumberofWorkingPersonsinHouseholdGeneralHouseholdSurvey2015.csv`\n",
    "  - Working persons by planning area (proxy for population distribution): `data/ResidentWorkingPersonsAged15YearsandOverbyPlanningAreaandUsualModeofTransporttoWorkGeneralHouseholdSurvey2015.csv`\n",
    "  - Mode to work by age/sex (additional demographic context): `data/ResidentWorkingPersonsAged15YearsandOverbyUsualModeofTransporttoWorkAgeGroupandSexGeneralHouseholdSurvey2015.csv`\n",
    "  - Students travel time (youth distribution proxy): `data/ResidentStudentsAged5YearsandOverbyTravellingTimetoSchoolLevelofEducationAttendingandSexGeneralHouseholdSurvey2015.csv`\n",
    "\n",
    "Note: If available, a direct \"Population by Planning Area (by age/sex)\" dataset would provide better denominators; otherwise, the above serve as proxies. Consider adding a SingStat population-by-planning-area file for current year.\n",
    "\n",
    "### Built-environment and transport covariates (optional but valuable)\n",
    "- Land use polygons (for land-use mix indices): `data/MasterPlan2019LandUselayer.geojson`\n",
    "- Rail/MRT stations:\n",
    "  - `data/AmendmenttoMasterPlan2019RailStationlayer.geojson`\n",
    "  - `data/LTAMRTStationExitGEOJSON.geojson`\n",
    "- School zones (youth-related context): `data/LTASchoolZone.geojson`\n",
    "\n",
    "### Socioeconomic proxies (optional)\n",
    "- Housing market proxies:\n",
    "  - `data/Resale Flat Prices (Based on Approval Date), 1990 - 1999.csv`\n",
    "  - `data/Resale Flat Prices (Based on Approval Date), 2000 - Feb 2012.csv`\n",
    "  - `data/Resale Flat Prices (Based on Registration Date), From Mar 2012 to Dec 2014.csv`\n",
    "  - `data/Resale Flat Prices (Based on Registration Date), From Jan 2015 to Dec 2016.csv`\n",
    "  - `data/Resale flat prices based on registration date from Jan-2017 onwards.csv`\n",
    "- Public transport usage (control): `data/PublicTransportUtilisationAveragePublicTransportRidership.csv`\n",
    "\n",
    "### Not required for this study (present but out-of-scope)\n",
    "- Industry/business sentiment and building energy datasets (e.g., manufacturing expectations, commercial building energy) unless used as broad economic context:\n",
    "  - `data/Business Expectations for the Next Three and Six Months, Manufacturing clusters.csv`\n",
    "  - `data/Business Expectations for the Next Three and Six Months, Manufacturing sub-clusters.csv`\n",
    "  - `data/Business Expectations for the Next Three and Six Months, Total Manufacturing.csv`\n",
    "  - `data/Listing of Building Energy Performance Data 2020.csv`\n",
    "  - `data/Listing of Building Energy Performance Data for Commercial Buildings.csv`\n",
    "\n",
    "### Minimal must-have subset (to run core pipeline)\n",
    "1) `SportSGSportFacilitiesGEOJSON.geojson`\n",
    "2) `MasterPlan2019PlanningAreaBoundaryNoSea.geojson`\n",
    "3) At least one population/denominator table (preferably population by planning area; otherwise the working-persons proxy)\n",
    "4) Participation CSVs (age/ethnicity/sex as available)\n",
    "5) `CyclingPathNetworkKML.kml` (for network-based metrics; can be skipped for Euclidean-only)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d3c9dfe",
   "metadata": {},
   "source": [
    "## Integrating SG Bus Data (cheeaun/sgbusdata)\n",
    "\n",
    "This project can leverage the SG Bus Data repo in two ways:\n",
    "\n",
    "- Online (no build): read clean JSON from the public data server `https://data.busrouter.sg/v1/`.\n",
    "- Offline (vendor a snapshot): clone/download `cheeaun/sgbusdata` and copy required JSON files from its `data/v1/` folder into `data/sgbusdata/` here.\n",
    "\n",
    "Notes:\n",
    "- Building the dataset from scratch (Node+scripts) requires an LTA Datamall API key and running the repo commands; you don’t need this if you just consume the published data.\n",
    "- We’ll primarily use bus stops (points) and optionally routes/patterns (lines) to create access features: nearest stop distance, stop counts within 400/800 m, and stop density per planning area.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "05c356f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-11-09T19:40:00.048493] Loaded 5175 bus stops from remote:stops.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_309840/3789452055.py:149: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  print(f\"[{datetime.utcnow().isoformat()}] Loaded {len(stops_df)} bus stops from {stops_origin}\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>code</th>\n",
       "      <th>lon</th>\n",
       "      <th>lat</th>\n",
       "      <th>name</th>\n",
       "      <th>road</th>\n",
       "      <th>__origin__</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10009</td>\n",
       "      <td>103.81722</td>\n",
       "      <td>1.28210</td>\n",
       "      <td>Bt Merah Int</td>\n",
       "      <td>Bt Merah Ctrl</td>\n",
       "      <td>remote:stops.json</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10011</td>\n",
       "      <td>103.83750</td>\n",
       "      <td>1.27774</td>\n",
       "      <td>Bef Neil Rd</td>\n",
       "      <td>New Bridge Rd</td>\n",
       "      <td>remote:stops.json</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10017</td>\n",
       "      <td>103.83763</td>\n",
       "      <td>1.27832</td>\n",
       "      <td>Aft Hosp Dr</td>\n",
       "      <td>Eu Tong Sen St</td>\n",
       "      <td>remote:stops.json</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10018</td>\n",
       "      <td>103.83860</td>\n",
       "      <td>1.27901</td>\n",
       "      <td>Outram Pk Stn Exit 6/SGH</td>\n",
       "      <td>Eu Tong Sen St</td>\n",
       "      <td>remote:stops.json</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10021</td>\n",
       "      <td>103.83839</td>\n",
       "      <td>1.27745</td>\n",
       "      <td>Blk 3</td>\n",
       "      <td>Neil Rd</td>\n",
       "      <td>remote:stops.json</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    code        lon      lat                      name            road  \\\n",
       "0  10009  103.81722  1.28210              Bt Merah Int   Bt Merah Ctrl   \n",
       "1  10011  103.83750  1.27774               Bef Neil Rd   New Bridge Rd   \n",
       "2  10017  103.83763  1.27832               Aft Hosp Dr  Eu Tong Sen St   \n",
       "3  10018  103.83860  1.27901  Outram Pk Stn Exit 6/SGH  Eu Tong Sen St   \n",
       "4  10021  103.83839  1.27745                     Blk 3         Neil Rd   \n",
       "\n",
       "          __origin__  \n",
       "0  remote:stops.json  \n",
       "1  remote:stops.json  \n",
       "2  remote:stops.json  \n",
       "3  remote:stops.json  \n",
       "4  remote:stops.json  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fac_df not found; define facilities (Cell 3) before running proximity metrics.\n",
      "plan_gdf not defined; skipping planning-area bus stop aggregation (run Cell 4 first).\n",
      "Loaded bus service/route metadata from remote:services.json\n",
      "Cell 6 completed.\n",
      "Loaded bus service/route metadata from remote:services.json\n",
      "Cell 6 completed.\n"
     ]
    }
   ],
   "source": [
    "# Cell 6: Load SG Bus Data (stops) and compute proximity & coverage metrics (improved + empty dataset handling + dict-mapping fix)\n",
    "\"\"\"\n",
    "Enhancements over initial version:\n",
    "- Robust handling if `fac_df` or `plan_gdf` are not yet in the session.\n",
    "- Faster nearest-stop distance using spatial index (R-tree) instead of unary union distance.\n",
    "- More efficient stop counts within radii using spatial index + filtering rather than per-facility buffer `.within` scans.\n",
    "- Optional integration of bus service route lines if available (to compute route coverage length per planning area and facility proximity to routes).\n",
    "- Graceful fallbacks, clear logging, and empty data protection.\n",
    "- NEW: Correctly parse `stops.json` structure (dict mapping code -> [lon, lat, name, road]) instead of assuming list or `{'stops': ...}` only.\n",
    "- NEW: Added additional candidate filenames (compressed / geojson variants) for stops data.\n",
    "\"\"\"\n",
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "import math\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "try:\n",
    "    import geopandas as gpd\n",
    "    from shapely.geometry import Point\n",
    "    from shapely.strtree import STRtree  # if shapely >=2; fallback to sindex for geopandas\n",
    "    GEOPANDAS_OK = True\n",
    "except Exception:\n",
    "    GEOPANDAS_OK = False\n",
    "\n",
    "BASE_URL = os.environ.get('SGBUSDATA_BASE_URL', 'https://data.busrouter.sg/v1')\n",
    "LOCAL_DIR = Path('..') / 'data' / 'sgbusdata' if Path('.').resolve().name == 'notebooks' else Path('data') / 'sgbusdata'\n",
    "CACHE_DIR = Path('..') / 'outputs' / 'cache' if Path('.').resolve().name == 'notebooks' else Path('outputs') / 'cache'\n",
    "CACHE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "STOP_FILES_CANDIDATES = [\n",
    "    'bus-stops.json',          # historical naming (often 404)\n",
    "    'stops.json',              # main JSON (dict mapping code->[lon,lat,name,road])\n",
    "    'stops.min.json',          # minified variant\n",
    "    'stops.geojson',           # geojson with FeatureCollection\n",
    "    'stops.min.geojson',       # minified geojson\n",
    "]\n",
    "ROUTE_FILES_CANDIDATES = [\n",
    "    'bus-services.json',       # list of services (sometimes)\n",
    "    'services.json',\n",
    "    'routes.json',             # route geometry (non-geojson)\n",
    "    'routes.geojson',          # geojson routes\n",
    "    'routes.min.geojson'\n",
    "]\n",
    "PATTERN_PREFIXES = [  # heuristic prefixes for pattern files; repository structure may differ\n",
    "    'patterns', 'routes'\n",
    "]\n",
    "\n",
    "\n",
    "def _http_get(url, timeout=20):\n",
    "    import requests\n",
    "    try:\n",
    "        r = requests.get(url, timeout=timeout)\n",
    "        if r.ok:\n",
    "            return r\n",
    "    except Exception:\n",
    "        return None\n",
    "    return None\n",
    "\n",
    "\n",
    "def load_json_candidates(base_url, local_dir, candidates):\n",
    "    \"\"\"Try remote then local for a list of candidate filenames; return first successful JSON object + origin.\"\"\"\n",
    "    for fname in candidates:\n",
    "        url = f\"{base_url.rstrip('/')}/{fname}\"\n",
    "        r = _http_get(url)\n",
    "        if r and r.ok:\n",
    "            # attempt parse regardless of content-type (server sometimes serves text/plain)\n",
    "            try:\n",
    "                return r.json(), f\"remote:{fname}\"\n",
    "            except Exception:\n",
    "                pass\n",
    "    for fname in candidates:\n",
    "        fp = local_dir / fname\n",
    "        if fp.exists():\n",
    "            try:\n",
    "                with open(fp, 'r', encoding='utf-8') as fh:\n",
    "                    return json.load(fh), f\"local:{fname}\"\n",
    "            except Exception:\n",
    "                pass\n",
    "    return None, None\n",
    "\n",
    "\n",
    "def load_stops():\n",
    "    data, origin = load_json_candidates(BASE_URL, LOCAL_DIR, STOP_FILES_CANDIDATES)\n",
    "    if data is None:\n",
    "        return pd.DataFrame(), origin\n",
    "\n",
    "    # Normalise to tabular rows\n",
    "    # Supported shapes:\n",
    "    # 1) {'stops': [...]} list of dicts\n",
    "    # 2) [ {...}, {...} ] list of dicts\n",
    "    # 3) { '01012': [lon, lat, name, road], ... } mapping code -> array\n",
    "    # 4) GeoJSON FeatureCollection (if stops.geojson)\n",
    "    items = []\n",
    "    if isinstance(data, dict):\n",
    "        if 'stops' in data and isinstance(data['stops'], list):\n",
    "            items = data['stops']\n",
    "        elif 'features' in data and isinstance(data['features'], list):  # geojson\n",
    "            for feat in data['features']:\n",
    "                props = feat.get('properties', {}) or {}\n",
    "                geom = feat.get('geometry') or {}\n",
    "                coords = geom.get('coordinates') if geom.get('type') == 'Point' else None\n",
    "                rec = {\n",
    "                    'code': props.get('code') or props.get('id') or props.get('BusStopCode'),\n",
    "                    'lon': coords[0] if coords else props.get('lon') or props.get('lng'),\n",
    "                    'lat': coords[1] if coords else props.get('lat'),\n",
    "                    'name': props.get('name') or props.get('Description'),\n",
    "                    'road': props.get('road') or props.get('Road')\n",
    "                }\n",
    "                items.append(rec)\n",
    "        else:\n",
    "            # Dict mapping code -> [lon, lat, name, road]\n",
    "            # Create structured records\n",
    "            sample_val = next(iter(data.values())) if data else []\n",
    "            if isinstance(sample_val, (list, tuple)) and len(sample_val) >= 2:\n",
    "                for code, arr in data.items():\n",
    "                    lon = arr[0] if len(arr) > 0 else None\n",
    "                    lat = arr[1] if len(arr) > 1 else None\n",
    "                    name = arr[2] if len(arr) > 2 else None\n",
    "                    road = arr[3] if len(arr) > 3 else None\n",
    "                    items.append({'code': code, 'lon': lon, 'lat': lat, 'name': name, 'road': road})\n",
    "            else:\n",
    "                # Fallback: treat as list of unknown structure\n",
    "                pass\n",
    "    elif isinstance(data, list):\n",
    "        items = data\n",
    "    # Build DataFrame\n",
    "    df = pd.DataFrame(items)\n",
    "\n",
    "    # Harmonise column names (create standard lon/lat if alternate names exist)\n",
    "    if 'longitude' in df.columns and 'lon' not in df.columns:\n",
    "        df['lon'] = df['longitude']\n",
    "    if 'latitude' in df.columns and 'lat' not in df.columns:\n",
    "        df['lat'] = df['latitude']\n",
    "    if 'lng' in df.columns and 'lon' not in df.columns:\n",
    "        df['lon'] = df['lng']\n",
    "\n",
    "    if not df.empty:\n",
    "        df['__origin__'] = origin\n",
    "    return df, origin\n",
    "\n",
    "\n",
    "stops_df, stops_origin = load_stops()\n",
    "if stops_df.empty:\n",
    "    print(f\"[WARN] Bus stops dataset is empty after parsing. Tried candidates: {STOP_FILES_CANDIDATES}. Origin: {stops_origin or 'none found'}\")\n",
    "    print(\"Set SGBUSDATA_BASE_URL env var or place a stops JSON/GeoJSON under data/sgbusdata/. Skipping bus stop proximity metrics.\")\n",
    "else:\n",
    "    print(f\"[{datetime.utcnow().isoformat()}] Loaded {len(stops_df)} bus stops from {stops_origin}\")\n",
    "    # Show simple head for verification\n",
    "    display(stops_df.head())\n",
    "\n",
    "# Heuristic column mapping (only proceed if data present)\n",
    "lat_col = next((c for c in stops_df.columns if c.lower() in ['lat','latitude']), None)\n",
    "lng_col = next((c for c in stops_df.columns if c.lower() in ['lon','lng','longitude']), None)\n",
    "name_col = next((c for c in stops_df.columns if c.lower() in ['name','stopname','description']), None)\n",
    "code_col = next((c for c in stops_df.columns if c.lower() in ['code','stopcode','busstopcode','id']), None)\n",
    "\n",
    "if stops_df.empty:\n",
    "    # Nothing further to compute\n",
    "    print('Skipping spatial proximity calculations due to empty stops dataset.')\n",
    "else:\n",
    "    if not GEOPANDAS_OK:\n",
    "        print('geopandas not available — skipping spatial metrics. Install geopandas to enable proximity calculations.')\n",
    "    else:\n",
    "        if not (lat_col and lng_col):\n",
    "            print('[WARN] Could not infer lat/lon columns from stops data; skipping proximity metrics.')\n",
    "        else:\n",
    "            g_stops = gpd.GeoDataFrame(\n",
    "                stops_df,\n",
    "                geometry=gpd.points_from_xy(stops_df[lng_col], stops_df[lat_col]),\n",
    "                crs='EPSG:4326'\n",
    "            )\n",
    "            target_crs = 'EPSG:3414'\n",
    "            g_stops = g_stops.to_crs(target_crs)\n",
    "\n",
    "            # Facility points from earlier parsing\n",
    "            if 'fac_df' not in globals():\n",
    "                print('fac_df not found; define facilities (Cell 3) before running proximity metrics.')\n",
    "            else:\n",
    "                pf = fac_df.dropna(subset=['centroid_lon','centroid_lat']).copy()\n",
    "                if pf.empty:\n",
    "                    print('[WARN] No facility points with valid centroids; skipping facility proximity metrics.')\n",
    "                else:\n",
    "                    g_fac_pts = gpd.GeoDataFrame(\n",
    "                        pf,\n",
    "                        geometry=gpd.points_from_xy(pf['centroid_lon'], pf['centroid_lat']),\n",
    "                        crs='EPSG:4326'\n",
    "                    ).to_crs(target_crs)\n",
    "\n",
    "                    # Build spatial index for stops\n",
    "                    sindex = g_stops.sindex\n",
    "\n",
    "                    def nearest_stop_distance(pt):\n",
    "                        possible_matches_index = list(sindex.nearest(pt.bounds, 1))\n",
    "                        if not possible_matches_index:\n",
    "                            return math.nan\n",
    "                        stop_geom = g_stops.geometry.iloc[possible_matches_index[0]]\n",
    "                        return pt.distance(stop_geom)\n",
    "\n",
    "                    g_fac_pts['dist_to_bus_stop_m'] = g_fac_pts.geometry.apply(nearest_stop_distance)\n",
    "\n",
    "                    def count_within_radius(pt, r):\n",
    "                        bounds = (pt.x - r, pt.y - r, pt.x + r, pt.y + r)\n",
    "                        candidate_idx = list(sindex.intersection(bounds))\n",
    "                        if not candidate_idx:\n",
    "                            return 0\n",
    "                        candidates = g_stops.geometry.iloc[candidate_idx]\n",
    "                        return int((candidates.distance(pt) <= r).sum())\n",
    "\n",
    "                    for r in [400, 800]:\n",
    "                        col = f'stops_within_{r}m'\n",
    "                        g_fac_pts[col] = g_fac_pts.geometry.apply(lambda p, rad=r: count_within_radius(p, rad))\n",
    "\n",
    "                    fac_bus_out = OUT_DIR / 'facilities_with_bus_proximity.csv'\n",
    "                    g_fac_pts.drop(columns='geometry').to_csv(fac_bus_out, index=False)\n",
    "                    print('Wrote', fac_bus_out)\n",
    "\n",
    "            # Planning-area aggregation (optional)\n",
    "            if 'plan_gdf' in globals():\n",
    "                try:\n",
    "                    plan_gdf = plan_gdf.to_crs(target_crs)\n",
    "                    stop_in_pa = gpd.sjoin(g_stops, plan_gdf[['planning_area','geometry']], how='left', predicate='within')\n",
    "                    pa_stop_counts = stop_in_pa.groupby('planning_area').size().reset_index(name='bus_stops_count')\n",
    "                    pa_aug = plan_gdf.merge(pa_stop_counts, on='planning_area', how='left')\n",
    "                    pa_aug['bus_stops_count'] = pa_aug['bus_stops_count'].fillna(0).astype(int)\n",
    "                    out_fp = OUT_DIR / 'planning_areas_with_bus_stops.geojson'\n",
    "                    pa_aug.to_file(out_fp, driver='GeoJSON')\n",
    "                    print('Wrote', out_fp)\n",
    "                except Exception as e:\n",
    "                    print('[WARN] Failed planning-area bus stop aggregation:', e)\n",
    "            else:\n",
    "                print('plan_gdf not defined; skipping planning-area bus stop aggregation (run Cell 4 first).')\n",
    "\n",
    "# Optional: attempt to load route/service data for advanced features (only if not empty)\n",
    "if not stops_df.empty:\n",
    "    route_data, route_origin = load_json_candidates(BASE_URL, LOCAL_DIR, ROUTE_FILES_CANDIDATES)\n",
    "    if route_data:\n",
    "        print(f\"Loaded bus service/route metadata from {route_origin}\")\n",
    "        with open(CACHE_DIR / 'bus_service_metadata.json', 'w', encoding='utf-8') as fh:\n",
    "            json.dump(route_data, fh)\n",
    "    else:\n",
    "        print('Bus service metadata not found (optional).')\n",
    "\n",
    "print('Cell 6 completed.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4bb7436",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "urban-transformer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
